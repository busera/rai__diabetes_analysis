{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up logging\n",
    "logger.add(\"../logs/model_training.log\", rotation=\"500 MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 12:46:40.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mStarting model training\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load the diabetes dataset from Parquet files.\"\"\"\n",
    "    X_train = pq.read_table('../data/interim/X_train.parquet').to_pandas()\n",
    "    X_val = pq.read_table('../data/interim/X_val.parquet').to_pandas()\n",
    "    X_test = pq.read_table('../data/interim/X_test.parquet').to_pandas()\n",
    "    y_train = pq.read_table('../data/interim/y_train.parquet').to_pandas()['target']\n",
    "    y_val = pq.read_table('../data/interim/y_val.parquet').to_pandas()['target']\n",
    "    y_test = pq.read_table('../data/interim/y_test.parquet').to_pandas()['target']\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 12:46:40.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mData loaded. Training set shape: (264, 10)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_data()\n",
    "logger.info(f\"Data loaded. Training set shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(X):\n",
    "    \"\"\"\n",
    "    Create a pipeline (compatible with the MS RAI Toolkit).\n",
    "\n",
    "    This function creates a scikit-learn pipeline that preprocesses both numeric\n",
    "    and categorical features, and includes a Random Forest Regressor as the final\n",
    "    estimator. The pipeline is designed to be compatible with the Responsible AI\n",
    "    (RAI) Toolkit.\n",
    "\n",
    "    Args:\n",
    "        X (pandas.DataFrame): The input features DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        sklearn.pipeline.Pipeline: A scikit-learn pipeline that includes\n",
    "        preprocessing steps for both numeric and categorical features,\n",
    "        followed by a Random Forest Regressor.\n",
    "\n",
    "    The pipeline includes the following steps:\n",
    "    1. Preprocessing:\n",
    "        a. For numeric features:\n",
    "            - Imputation of missing values with median strategy\n",
    "            - Standard scaling\n",
    "            - Discretization using K-bins with 10 bins\n",
    "        b. For categorical features:\n",
    "            - Imputation of missing values with 'missing' as the fill value\n",
    "            - One-hot encoding\n",
    "    2. Random Forest Regressor with 1000 estimators\n",
    "\n",
    "    Note:\n",
    "        The function automatically detects numeric and categorical columns\n",
    "        in the input DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        #('discretizer', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile'))\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(n_estimators=1000, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 12:46:48.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mModel trained\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model = create_pipeline(X_train)\n",
    "model.fit(X_train, y_train)\n",
    "logger.info(\"Model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, dataset_name):\n",
    "    \"\"\"Evaluate the model and log the results.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    logger.info(f\"{dataset_name} - MSE: {mse:.4f}, R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 12:46:48.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mTraining - MSE: 489.7641, R2: 0.9212\u001b[0m\n",
      "\u001b[32m2024-08-08 12:46:49.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mValidation - MSE: 2933.4075, R2: 0.4646\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 12:46:49.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mTest - MSE: 3044.6671, R2: 0.4253\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(model, X_train, y_train, \"Training\")\n",
    "evaluate_model(model, X_val, y_val, \"Validation\")\n",
    "evaluate_model(model, X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 12:46:49.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mTop 5 important features:\u001b[0m\n",
      "\u001b[32m2024-08-08 12:46:49.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mfeature  importance\n",
      "    bmi    0.354923\n",
      "     s5    0.238905\n",
      "     bp    0.087345\n",
      "     s6    0.066214\n",
      "    age    0.062073\u001b[0m\n",
      "\u001b[32m2024-08-08 12:46:49.700\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[32m\u001b[1mModel training and evaluation complete.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance\n",
    "feature_names = X_train.columns.tolist()\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': model.named_steps['regressor'].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "logger.info(\"Top 5 important features:\")\n",
    "logger.info(feature_importance.head().to_string(index=False))\n",
    "\n",
    "logger.success(\"Model training and evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibleai import RAIInsights\n",
    "from raiwidgets import ResponsibleAIDashboard\n",
    "from responsibleai.feature_metadata import FeatureMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rai_data(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Prepare data for RAI Insights.\n",
    "    \n",
    "    Args:\n",
    "    model: Trained model\n",
    "    X_test: Test features\n",
    "    y_test: Test target\n",
    "    \n",
    "    Returns:\n",
    "    Tuple of (model, X_test, y_test) prepared for RAI Insights\n",
    "    \"\"\"\n",
    "    print(\"Shape of X_test:\", X_test.shape)\n",
    "    print(\"Shape of y_test:\", y_test.shape)\n",
    "    \n",
    "    # Ensure X_test is a DataFrame\n",
    "    if not isinstance(X_test, pd.DataFrame):\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "    \n",
    "    # Ensure y_test is a DataFrame\n",
    "    if not isinstance(y_test, pd.DataFrame):\n",
    "        y_test = pd.DataFrame(y_test, columns=['target'])\n",
    "    \n",
    "    print(\"Shape of y_test after conversion:\", y_test.shape)\n",
    "    \n",
    "    # Reset index of X_test and y_test\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    # Combine X_test and y_test into a single DataFrame\n",
    "    test_df = pd.concat([X_test, y_test], axis=1)\n",
    "    \n",
    "    print(\"Shape of final test_df:\", test_df.shape)\n",
    "    print(\"Columns in test_df:\", test_df.columns)\n",
    "    print(\"First few rows of 'target' column:\")\n",
    "    print(test_df['target'].head())\n",
    "    \n",
    "    return model, test_df, 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rai_insights(model, X_test, y_test, task_type='regression'):\n",
    "    \"\"\"\n",
    "    Create and compute comprehensive RAI Insights.\n",
    "    \n",
    "    Args:\n",
    "    model: Trained model\n",
    "    X_test: Test features\n",
    "    y_test: Test target\n",
    "    task_type: 'regression' or 'classification'\n",
    "    \n",
    "    Returns:\n",
    "    RAIInsights object\n",
    "    \"\"\"\n",
    "    # Prepare data for RAI Insights\n",
    "    model, test_df, target_column = prepare_rai_data(model, X_test, y_test)\n",
    "    \n",
    "    print(\"Shape of test_df after prepare_rai_data:\", test_df.shape)\n",
    "    print(\"Columns in test_df:\", test_df.columns)\n",
    "    print(\"First few rows of 'target' column:\")\n",
    "    print(test_df[target_column].head())\n",
    "    \n",
    "    # Define feature metadata\n",
    "    feature_metadata = FeatureMetadata()\n",
    "    # Add your categorical features here, for example:\n",
    "    feature_metadata.add_categorical_feature('bmi')\n",
    "    \n",
    "    # Create RAI Insights object\n",
    "    rai_insights = RAIInsights(model, test_df, test_df, target_column,\n",
    "                               task_type=task_type,\n",
    "                               feature_metadata=feature_metadata)\n",
    "    \n",
    "    # Add explainer\n",
    "    rai_insights.explainer.add()\n",
    "    \n",
    "    # Add error analysis\n",
    "    rai_insights.error_analysis.add()\n",
    "    \n",
    "    # Add causal analysis\n",
    "    # Note: This requires specific treatment features\n",
    "    rai_insights.causal.add(treatment_features=['bmi', 'bp', 's5'])\n",
    "    \n",
    "    # Add counterfactuals\n",
    "    if task_type == 'regression':\n",
    "        # For regression, we need to specify a desired range\n",
    "        y_min, y_max = test_df[target_column].min(), test_df[target_column].max()\n",
    "        y_range = y_max - y_min\n",
    "        desired_range = [y_min - 0.1 * y_range, y_max + 0.1 * y_range]  # Extend slightly beyond observed range\n",
    "        rai_insights.counterfactual.add(total_CFs=10, desired_range=desired_range)\n",
    "    else:\n",
    "        rai_insights.counterfactual.add(total_CFs=10)\n",
    "    \n",
    "    # Add data balance measures only for classification tasks\n",
    "    if task_type == 'classification':\n",
    "        cols_of_interest = [col for col in test_df.columns if col != target_column]\n",
    "        rai_insights.data_balance.add(cols_of_interest=cols_of_interest)\n",
    "    \n",
    "    # Compute insights\n",
    "    rai_insights.compute()\n",
    "    \n",
    "    return rai_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (89, 10)\n",
      "Shape of y_test: (89,)\n",
      "Shape of y_test after conversion: (89, 1)\n",
      "Shape of final test_df: (89, 11)\n",
      "Columns in test_df: Index(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6',\n",
      "       'target'],\n",
      "      dtype='object')\n",
      "First few rows of 'target' column:\n",
      "0    219.0\n",
      "1     70.0\n",
      "2    202.0\n",
      "3    230.0\n",
      "4    111.0\n",
      "Name: target, dtype: float64\n",
      "Shape of test_df after prepare_rai_data: (89, 11)\n",
      "Columns in test_df: Index(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6',\n",
      "       'target'],\n",
      "      dtype='object')\n",
      "First few rows of 'target' column:\n",
      "0    219.0\n",
      "1     70.0\n",
      "2    202.0\n",
      "3    230.0\n",
      "4    111.0\n",
      "Name: target, dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Causal Effects\n",
      "Current Status: Generating Causal Effects.\n",
      "Current Status: Finished generating causal effects.\n",
      "Time taken: 0.0 min 14.611036987000034 sec\n",
      "================================================================================\n",
      "================================================================================\n",
      "Counterfactual\n",
      "Current Status: Generating 10 counterfactuals for 89 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [06:08<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Status: Generated 10 counterfactuals for 89 samples.\n",
      "Time taken: 6.0 min 13.077335656999821 sec\n",
      "================================================================================\n",
      "================================================================================\n",
      "Error Analysis\n",
      "Current Status: Generating error analysis reports.\n",
      "Current Status: Finished generating error analysis reports.\n",
      "Time taken: 0.0 min 1.5836292359999788 sec\n",
      "================================================================================\n",
      "================================================================================\n",
      "Explanations\n",
      "Current Status: Explaining 10 features\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 222\n",
      "[LightGBM] [Info] Number of data points in the train set: 89, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 145.304696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 12:53:21.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mRAI Insights computed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Status: Explained 10 features.\n",
      "Time taken: 0.0 min 1.4240740999998707 sec\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-08 12:53:23,660] WARNING in __init__: WebSocket transport not available. Install gevent-websocket for improved performance.\n",
      "\u001b[32m2024-08-08 12:53:25.021\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[32m\u001b[1mRAI Dashboard launched. Model training and analysis complete.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponsibleAI started at http://localhost:8705\n"
     ]
    }
   ],
   "source": [
    "# Create and compute RAI Insights\n",
    "rai_insights = create_rai_insights(model, X_test, y_test)\n",
    "logger.info(\"RAI Insights computed\")\n",
    "\n",
    "# Display the RAI Dashboard\n",
    "ResponsibleAIDashboard(rai_insights)\n",
    "logger.success(\"RAI Dashboard launched. Model training and analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diabetes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
